<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Optimising Quote Retrieval: How AQE Finds Better Needles in Academic Haystacks | Foundry of Zero</title><meta name=description content="The search pipeline in Academic Quote Extractor was leaving good quotes on the table. Here's how query expansion and batched scoring fixed the retrieval problem from both ends."><meta property="og:title" content="Optimising Quote Retrieval: How AQE Finds Better Needles in Academic Haystacks"><meta property="og:description" content="The search pipeline in Academic Quote Extractor was leaving good quotes on the table. Here's how query expansion and batched scoring fixed the retrieval problem from both ends."><meta property="og:url" content="https://foundryofzero.ai/blog/batched-scoring-aqe/"><meta property="og:site_name" content="Foundry of Zero"><meta property="og:type" content="article"><meta property="og:image" content="https://foundryofzero.ai/images/og-default.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Optimising Quote Retrieval: How AQE Finds Better Needles in Academic Haystacks"><meta name=twitter:description content="The search pipeline in Academic Quote Extractor was leaving good quotes on the table. Here's how query expansion and batched scoring fixed the retrieval problem from both ends."><meta name=twitter:image content="https://foundryofzero.ai/images/og-default.png"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500;700&display=swap" rel=stylesheet><link rel=stylesheet href=/css/main.css><link rel=icon type=image/svg+xml href=/favicon.svg></head><body><nav class=site-nav role=navigation aria-label="Main navigation"><div class=nav-container><a href=/ class=nav-logo aria-label=Home>FoZ</a><ul class=nav-links><li><a href=/projects/>Projects</a></li><li><a href=/blog/>Blog</a></li><li><a href=/about/>About</a></li><li><a href=https://github.com/nixlim target=_blank rel="noopener noreferrer" class=nav-github aria-label=GitHub><svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg></a></li></ul><button class=nav-hamburger aria-label="Toggle menu" aria-expanded=false aria-controls=mobile-menu>
<span class=hamburger-line></span>
<span class=hamburger-line></span>
<span class=hamburger-line></span></button></div><div class=mobile-menu id=mobile-menu aria-hidden=true><ul class=mobile-nav-links><li><a href=/projects/>Projects</a></li><li><a href=/blog/>Blog</a></li><li><a href=/about/>About</a></li><li><a href=https://github.com/nixlim target=_blank rel="noopener noreferrer" aria-label=GitHub>GitHub</a></li></ul></div></nav><main id=content><div class=container><article class=blog-post><h1>Optimising Quote Retrieval: How AQE Finds Better Needles in Academic Haystacks</h1><div class=post-meta-row><time datetime=2026-02-01>February 1, 2026</time>
<span class=text-muted>&#183;</span>
<span class=text-muted>6 min read</span>
<span class=text-muted>&#183;</span>
<a href=/tags/aqe/ class=tag-pill>aqe</a>
<a href=/tags/architecture/ class=tag-pill>architecture</a>
<a href=/tags/llm/ class=tag-pill>llm</a>
<a href=/tags/rag/ class=tag-pill>rag</a>
<a href=/tags/vibe-coding/ class=tag-pill>vibe-coding</a></div><div class=content><img src=/images/getting_better_quotes.png alt="Getting better quotes from academic papers"><p>Academic Quote Extractor has a deceptively simple job: you give it a research topic, it gives you real quotes from real papers with real citations. Under the hood, it&rsquo;s a hybrid RAG pipeline &ndash; Weaviate for retrieval, Claude for relevance scoring, SQLite for ground truth. It worked. The quotes came back correct, the citations were verifiable, and nobody was hallucinating references.</p><p>But &ldquo;correct&rdquo; and &ldquo;comprehensive&rdquo; are different things. The pipeline was leaving good quotes on the table, and it took an audit of the entire search flow to understand why.</p><h2 id=the-gaps-in-the-pipeline>The gaps in the pipeline</h2><p>When we laid out the current search pipeline and asked &ldquo;where are quotes getting lost?&rdquo;, the answer was: basically everywhere between the user&rsquo;s topic and Claude&rsquo;s scoring prompt.</p><table><thead><tr><th>Area</th><th>What was happening</th><th>What it cost us</th></tr></thead><tbody><tr><td><strong>Query</strong></td><td>User&rsquo;s raw topic string passed verbatim to Weaviate</td><td>BM25 depends on keyword overlap. &ldquo;Impact of social media on political polarization&rdquo; won&rsquo;t match chunks that say &ldquo;online platforms&rdquo; or &ldquo;partisan divide.&rdquo; Those quotes just vanish.</td></tr><tr><td><strong>Alpha</strong></td><td>Hardcoded 0.5 (equal BM25/vector weight)</td><td>Probably not optimal for academic text. Vector search tends to outperform keyword search for conceptual queries, but we were giving them equal say.</td></tr><tr><td><strong>Fusion</strong></td><td>Default <code>rankedFusion</code></td><td><code>relativeScoreFusion</code> normalises scores before combining, which can give better results when BM25 and vector scores are on different scales.</td></tr><tr><td><strong>Query expansion</strong></td><td>None. Topic goes straight to Weaviate.</td><td>Missing synonym and concept matches entirely. A chunk about &ldquo;computational linguistics&rdquo; won&rsquo;t surface for a query about &ldquo;natural language processing&rdquo; unless the vector embeddings happen to be close enough &ndash; and <code>nomic-embed-text</code> at 137M parameters doesn&rsquo;t catch every semantic relationship.</td></tr><tr><td><strong>Scoring capacity</strong></td><td>All candidates crammed into one Claude call</td><td>30 chunks x ~3,200 chars = ~96K characters per prompt. Already substantial. At 100 candidates, that&rsquo;s ~320K characters, and the Claude CLI subprocess ceases to acknowledge the concept of functioning. This hard-caps how many candidates we can even evaluate.</td></tr></tbody></table><p>The first two problems are about <em>finding</em> good candidates. The last one is about <em>evaluating</em> them. They&rsquo;re complementary failures, and fixing one without the other only gets you halfway.</p><h2 id=tier-1-query-expansion--finding-quotes-that-keywords-miss>Tier 1: Query expansion &ndash; finding quotes that keywords miss</h2><p>The highest-impact fix turned out to be the simplest conceptually. Before searching Weaviate, make a quick Claude call to expand the user&rsquo;s topic into multiple search-optimised queries:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-fallback data-lang=fallback><span style=display:flex><span>User topic: &#34;programming languages and software engineering&#34;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Claude generates:
</span></span><span style=display:flex><span>  → &#34;programming languages and software engineering&#34; (original)
</span></span><span style=display:flex><span>  → &#34;programming paradigms language design syntax semantics&#34;
</span></span><span style=display:flex><span>  → &#34;software development methodology code quality testing&#34;
</span></span></code></pre></div><p>Then run a Weaviate search for each variant, union the results, and deduplicate by chunk ID. The original query still runs &ndash; you&rsquo;re not replacing it, you&rsquo;re supplementing it. The expanded queries catch the synonym misses that BM25 can&rsquo;t handle and that a 137M-parameter embedding model doesn&rsquo;t always bridge.</p><p>Cost: one small Claude call (~$0.01) for expansion, plus 2-3 extra Weaviate searches that are free and local. The retrieval pool goes from &ldquo;whatever matched your exact keywords plus nearby vectors&rdquo; to &ldquo;everything conceptually related to your topic across the entire corpus.&rdquo;</p><p>This is where the biggest quality gains live. You can&rsquo;t score a quote Claude never sees.</p><h2 id=tier-2-batched-scoring--evaluating-all-of-them-without-dying>Tier 2: Batched scoring &ndash; evaluating all of them without dying</h2><p>Query expansion surfaces more candidates. Good. But now you&rsquo;ve got 60-100 chunks to score, and the prompt size ceiling hasn&rsquo;t moved. This is where batching comes in.</p><p>Instead of one massive Claude call, split the candidates into batches of 30 and score each batch independently. Merge the results. Sort by relevance.</p><p>The prompt template doesn&rsquo;t change. Search doesn&rsquo;t change. Storage doesn&rsquo;t change. ~70 lines of new Go code, no re-ingestion, no schema changes.</p><p>These two improvements work best together. Expansion finds better candidates. Batching evaluates all of them. Attack the problem from both ends.</p><h2 id=the-combined-flow>The combined flow</h2><p>The optimised pipeline looks like this:</p><ol><li><strong>Claude expands the topic</strong> into 3-4 search queries (~1s, ~$0.01)</li><li><strong>Weaviate runs each query</strong>, unions results, deduplicates by chunk ID &ndash; surfaces 60-100 unique candidates instead of 30</li><li><strong>Claude scores them in batches of 30</strong> &ndash; evaluates all candidates without OOM</li><li><strong>Filter and present</strong> as before &ndash; top quotes, verbatim, with citations</li></ol><p>Step 1 ensures you&rsquo;re not missing quotes because of keyword mismatch. Step 3 ensures you can actually evaluate everything step 2 found. Neither alone solves the quality problem; together they cover it.</p><h2 id=the-trade-offs>The trade-offs</h2><p>Batching isn&rsquo;t free. Neither is query expansion. Here&rsquo;s the honest accounting:</p><table><thead><tr><th>Factor</th><th>What actually happens</th></tr></thead><tbody><tr><td><strong>Cost</strong></td><td>Query expansion adds ~$0.01 per extraction. Batched scoring at 100 candidates = 4 Claude calls instead of 1. Your API bill scales linearly with batch count. Still cheap, but not zero.</td></tr><tr><td><strong>Latency</strong></td><td>Expansion adds ~1 second. Batches run sequentially at ~15-20s each. 4 batches = 60-80s total vs ~20s for a single call. The user waits longer. Worth it if the quotes are better.</td></tr><tr><td><strong>Score consistency</strong></td><td>Each batch scores independently. A chunk scoring 75 in batch 1 might have scored 70 in batch 2 because it&rsquo;s competing against different neighbours. The LLM&rsquo;s relevance judgments are influenced by what else is in the prompt. In practice, the variance is minor &ndash; a few points &ndash; and the final sort across all batches still surfaces the good stuff. But the scores aren&rsquo;t perfectly comparable across batches.</td></tr><tr><td><strong>Failure handling</strong></td><td>If batch 3 of 4 fails, we fail entirely. Partial results from a relevance-ranked pipeline are misleading &ndash; you&rsquo;d think you got the best quotes, but you only searched 75% of the candidate space.</td></tr></tbody></table><h2 id=what-we-didnt-do-and-why>What we didn&rsquo;t do (and why)</h2><p>Some ideas looked good on paper but weren&rsquo;t worth the complexity:</p><ul><li><strong>Weaviate reranker module</strong> &ndash; Requires changing the Docker setup and pulling another model. Claude is already doing the reranking externally, and doing it better than a generic reranker would on academic text.</li><li><strong>BM25 parameter tuning</strong> (k1, b values) &ndash; Marginal gains for the effort. Vector search compensates for BM25&rsquo;s weaknesses, and query expansion addresses the keyword mismatch problem more directly.</li><li><strong>Document-level filtering</strong> &ndash; A nice scoping feature (&ldquo;only search this specific paper&rdquo;) but it doesn&rsquo;t improve retrieval quality. It&rsquo;s a UX improvement, not a relevance improvement. Backlog.</li><li><strong>Alpha tuning + fusion type</strong> &ndash; Changing the BM25/vector weight from 0.5 to maybe 0.6-0.7 (biasing toward vector for semantic queries), and switching from <code>rankedFusion</code> to <code>relativeScoreFusion</code>. These are one-line changes each. Trivial to implement, but they need experimentation to find optimal values. We&rsquo;ll expose <code>--alpha</code> as a CLI flag and let users tune it for their corpus.</li></ul><h2 id=the-meta-lesson>The meta-lesson</h2><p>Every RAG system hits a scaling wall. The wall is usually not where you expect it. It&rsquo;s not in the vector search (that scales fine). It&rsquo;s not in the embedding generation (that&rsquo;s embarrassingly parallel). It&rsquo;s in two places nobody warns you about: the retrieval query itself, and the scoring prompt that the candidates get stuffed into.</p><p>The retrieval query problem is that users write topics in natural language, but search engines &ndash; even hybrid ones &ndash; need help bridging the vocabulary gap. A one-dollar LLM call for query expansion buys you more quality improvement than any amount of BM25 parameter tuning.</p><p>The scoring prompt problem is pure arithmetic. More candidates = bigger prompt = dead subprocess. Batching is the obvious fix, with the non-obvious caveat that cross-batch score comparisons are approximate rather than exact.</p><p>Fix retrieval to find better candidates. Fix scoring to evaluate more of them. The quotes get better from both directions, and neither change requires re-ingesting your corpus or redesigning your schema. That&rsquo;s the kind of optimisation that makes you wonder why you didn&rsquo;t do it on day one &ndash; which, of course, is what day two is for.</p></div><a href=/blog/ class=back-link>&larr; Back to Blog</a></article></div></main><footer class=site-footer><div class=footer-container><p class=footer-tagline>Because why not?</p><p class=footer-copyright>&copy; 2026 Foundry of Zero.
<a href=https://github.com/nixlim target=_blank rel="noopener noreferrer">GitHub</a></p><p class=footer-credit>Vibe-coded with <a href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a>. Because why not.</p></div></footer><script src=/js/nav.js defer></script></body></html>