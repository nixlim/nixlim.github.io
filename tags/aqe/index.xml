<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Aqe on Foundry of Zero</title><link>https://foundryofzero.ai/tags/aqe/</link><description>Recent content in Aqe on Foundry of Zero</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 01 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://foundryofzero.ai/tags/aqe/index.xml" rel="self" type="application/rss+xml"/><item><title>Optimising Quote Retrieval: How AQE Finds Better Needles in Academic Haystacks</title><link>https://foundryofzero.ai/blog/batched-scoring-aqe/</link><pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate><guid>https://foundryofzero.ai/blog/batched-scoring-aqe/</guid><description>&lt;img src="https://foundryofzero.ai/images/getting_better_quotes.png" alt="Getting better quotes from academic papers"&gt;
&lt;p&gt;Academic Quote Extractor has a deceptively simple job: you give it a research topic, it gives you real quotes from real papers with real citations. Under the hood, it&amp;rsquo;s a hybrid RAG pipeline &amp;ndash; Weaviate for retrieval, Claude for relevance scoring, SQLite for ground truth. It worked. The quotes came back correct, the citations were verifiable, and nobody was hallucinating references.&lt;/p&gt;
&lt;p&gt;But &amp;ldquo;correct&amp;rdquo; and &amp;ldquo;comprehensive&amp;rdquo; are different things. The pipeline was leaving good quotes on the table, and it took an audit of the entire search flow to understand why.&lt;/p&gt;</description></item></channel></rss>